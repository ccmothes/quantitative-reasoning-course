# Working with Qualitative Data in R

The main package we use to analyze qualitative (i.e, non-numeric) data in R is [`tidytext`](https://juliasilge.github.io/tidytext/), which by the name you may have already guessed is designed to work with `tidyverse` packages and tidy data principles.

Before starting make sure to `install.packages("tidytext")`. Also, if you want to make a word cloud visualization later in this tutorial, also install the `wordcloud` package. Then, read in the libraries:

```{r eval =TRUE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(wordcloud)
```

You should download the `Qual Methods Survey.xlsx` file from Canvas and put it in the `data/` folder you've been using throughout class (within an R Project). Then run the following chunk of code, which uses `readxl` (which should already be installed with RStudio) to read in an Excel file. We also have to specify which sheet of the excel file to read in.

```{r eval = TRUE}
data <- readxl::read_excel("data/Qual Methods Survey.xlsx", 
    sheet = "Form1")

```

Now, let's analyze one question at a time.

### Is Science Objective?

The first one was a short, 'Yes' or 'No' in response to the question 'Do you think science is objective?'.

We can make a quick plot to summarize the responses:

```{r eval = TRUE}
data %>% 
  ggplot(aes(x = `Do you think science is objective?`))+
  geom_bar()+
  #adds the actual count value to the chart
  geom_text(aes(label = after_stat(count)), stat = "count", vjust = 1.5, size = 12, color = "white")
```

**Note that since we have spaces in our column headers, we must put the title within \` \` or " ".**

Now the next question was an open ended follow up, "Why or Why Not?"

Before we conduct the text analysis, lets split our data in two, those that said 'yes' and those that said 'no'.

```{r eval = TRUE}
yes <- data %>% 
  filter(`Do you think science is objective?` == "Yes")


no <- data %>% 
  filter(`Do you think science is objective?` == "No")
```

Let's analyze the 'Yes' responses first.

First, we always set up the text analysis by using the `tidytext` function `unnest_tokens()` which will tokenize the text for us, meaning taking the full responses and separating each word out into its own row becoming a unique observation. You can also separate responses by consecutive words (i.e., ngrams), sentences, and more by changing the `token =`argument, which we will do later.

There is a second step we want to add to this process, which is to remove 'stop words', removing noise in the data. `tidytext` has a built in data frame of these stop words in English called `stop_words`.

```{r eval = TRUE}
stop_words
```

We remove these stop words from our data frame with the `anti_join()` function, which keeps all the words that are NOT found in `stop_words`. To easily `anti_join()`, we want to also name the new text column we create from `unnest_tokens()` `word`.

So, to prepare our data set for text analysis the code looks like this:

```{r eval = TRUE}
yes_why <- yes %>%
  #keep just our column of interest
  select(`Why or why not?`) %>% 
  unnest_tokens(output = word, #the new column name to put the text in
                input = `Why or why not?`)  %>%
  anti_join(stop_words, by = "word") # remove any stop words
```

```{r}
yes_why
```

Let's do some summary stats of these responses:

```{r eval = TRUE}
yes_why %>% 
  count(word, sort = TRUE)
```

We see a few most common words stand out. Let's visualize this, and since we still have 96 words lets visualize the words the come up more than once:

```{r eval = TRUE}
yes_why %>% 
  count(word) %>% 
  filter(n >1) %>% 
  ggplot(aes(x = reorder(word,n), y = n))+ #reorder makes the bars go in order low to high by a variable
  geom_col()+
  theme(axis.text.x = element_text(angle = 45))
```

Now lets do the same for the "No" responses and compare:

```{r eval = TRUE}
no_why <- no %>% 
  select(`Why or why not?`) %>% 
  unnest_tokens(output = word, #the new column name to put the text in
                input = `Why or why not?`)  %>%
  anti_join(stop_words, by = "word")
  
```

Snapshot of the word summary:

```{r eval = TRUE}
no_why %>% 
  count(word, sort = TRUE)
```

```{r eval = TRUE}
no_why %>% 
  count(word) %>% 
  filter(n >1) %>% 
  ggplot(aes(x = reorder(word,n), y = n))+ #reorder makes the bars go in order high to low
  geom_col()+
  scale_y_continuous(expand = c(0,0))+
  theme(axis.text.x = element_text(angle = 45))
```

Let's compare the top 5 words in "Yes" vs. "No" by binding our dataframes and faceting:

```{r eval = TRUE, message=FALSE}
yes_summary <- yes_why %>% 
  count(word) %>% 
  # take the top 5
  top_n(5) %>% 
  # create a new variable we can facet by later
  mutate(answer = "Yes")


# do the same for No
no_summary <- no_why %>% 
  count(word) %>% 
  # take the top 5
  top_n(5) %>% 
  # create a new variable we can facet by later
  mutate(answer = "No")
  
```

Now bind these into one data frame and compare the answers

```{r eval = TRUE}
bind_rows(yes_summary, no_summary) %>% 
  ggplot(aes(x = reorder(word,n), y = n))+
  geom_col()+
  facet_wrap(~answer)+
  theme(axis.text.x = element_text(angle = 45))
```

Another way to compare the answers is to calculate the proportion of each word in the dataset and create a correlation plot

```{r eval=TRUE, warning=FALSE}
bind_rows(mutate(yes_why, answer = "yes"),
          mutate(no_why, answer = "no"))%>% 
  group_by(answer) %>% 
  count(word) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = answer, values_from = proportion) %>% 
  ggplot(aes(x = no, y = yes))+
  geom_jitter(color = "black")+
  geom_text(aes(label = word), color = "black", check_overlap = TRUE, vjust = 1)
```

### What are the pros and cons of open science?

Next, let's analyze the responses describing the pros and cons to open science.

For this example let's compare the responses using n-grams, which looks at adjacent words instead of just single words, so we can detect common phrases and word associations. The process is similar as before, using the `unnest_tokens()` function, but this time we add the argument `token = "ngrams"`. We also specify `n` for how many consecutive words to examine, starting with an `n = 2` argument which is often called a 'bigram'.

Let's start analyzing the *pros* of open science:

```{r eval=TRUE}
pros_bigrams <- data %>% 
  select(ID, text = `What do you think are the pros of open science?`) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

Examine the most common pairs of words:

```{r eval=TRUE}
pros_bigrams %>% 
  count(bigram, sort = TRUE)
```

Let's clean this us by removing stop words. Since we now have a column with two word strings instead of one, we have to clean this a little differently. First, we use `separate()` to convert our single column into two, and specify that the empty space is our separator. Then we filter out stop words from both columns.

```{r eval=TRUE}
pros_bigrams %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)
```

Now we can see the most common pairs of words in the responses that don't contain noise/stop words. Lets do the same for the cons of open science:

```{r eval =TRUE}
cons_biograms <- data %>% 
  select(ID, text = `What do you think are the cons of open science?`) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

Now clean and summarize:

```{r eval=TRUE}
cons_biograms %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)
```

### Contribute to Equity and Environmental Justice

Lastly, let's work with the survey question 'In what ways do you believe you can contribute to equity and environmental justice?'

Let's first analyze the most common words in the data set, and make a word cloud using the `wordcloud` package.

```{r eval=TRUE}
data %>% 
  select(`In what ways do you believe you (in any aspect of your life or career) can contribute to equity and environmental justice?`) %>% 
  unnest_tokens(output = word, #the new column name to put the text in
                input = `In what ways do you believe you (in any aspect of your life or career) can contribute to equity and environmental justice?`)  %>%
  anti_join(stop_words, by = "word") %>% 
  count(word) %>% 
  # make the wordcloud
  with(wordcloud(
    words = word,
    freq = n,
    random.order = FALSE,
    scale = c(2, 0.5),
    min.freq = 2,
    max.words = 100,
    colors = c("#6FA8F5",
               "#FF4D45",
               "#FFC85E")
  ))
```

Looks like the most common words are pretty repetitive of those in the original question. Lets look at the ngrams for these responses, and try looking at phrases by pulling three consecutive words at a time.

```{r eval=TRUE}
data %>% 
  select(ID, text = `In what ways do you believe you (in any aspect of your life or career) can contribute to equity and environmental justice?`) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 3) %>% 
  separate(bigram, into = c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)
```

## More Resources

There's a lot more you can do with qualitative data in R. I recommend checking out the `tidytext` [website](https://juliasilge.github.io/tidytext/index.html) and [book](https://www.tidytextmining.com/) for more things you can do with the package, such as sentiment analysis (identifying positive and negative words in a quantitative way), topic modeling, and more.
