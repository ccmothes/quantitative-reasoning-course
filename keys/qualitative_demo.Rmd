# Working with Qualitative Data in R

```{r}
library(tidyverse)
library(tidytext)
```

```{r}
data <- readxl::read_excel("keys/Qual Methods Survey.xlsx", 
    sheet = "Form1")

```

Let's analyze one question at a time.

### Is Science Objective?

The first one was a short, 'Yes' or 'No' in response to the question 'Do you think science is objective?'.

We can make a quick plot to summarize your responses:

```{r}
dat %>% 
  ggplot(aes(x = `Do you think science is objective?`))+
  geom_bar()+
  #adds the actual count value to the chart
  geom_text(aes(label = after_stat(count)), stat = "count", vjust = 1.5, size = 12, color = "white")
```

Note that since we have spaces in our column headers, we must put the title within \` \` or " ".

Now the next question was an open ended follow up, "Why or Why Not?"

Before we conduct thet text analysis, lets split our data in two, those that said 'yes' and those that said 'no'.

```{r}
yes <- dat %>% 
  filter(`Do you think science is objective?` == "Yes")


no <- dat %>% 
  filter(`Do you think science is objective?` == "No")
```

Let's analyze the 'Yes' responses first.

First, we always set up the text analysis by using the `tidytext` function `unnest_tokens()` which will tokenize the text for us, meaning taking the full sentence responses and separating each word out into its own row becoming a unique observation.

There is a second step we want to add to this process, which is to remove 'stop words', removing noise in the data. `tidytext` has a built in data frame of these stop words in English called `stop_words`.

```{r}
stop_words
```

We remove these stop words from our data frame with the `anti_join()` function, which keeps all the words that are NOT found in `stop_words`. To easily `anti_join()`, we want to also name the new text column `word`.

So, to prepare our data set for text analysis the code looks like this:

```{r}
yes_why <- yes %>%
  #keep just our column of interest
  select(`Why or why not?`) %>% 
  unnest_tokens(output = word, #the new column name to put the text in
                input = `Why or why not?`)  %>%
  anti_join(stop_words)
```

```{r}
yes_why
```

Let's do some summary stats of these responses:

```{r}
yes_why %>% 
  count(word, sort = TRUE)
```

We see a few most common words stand out. Let's visualize this, and since we still have 96 words lets visualize the words the come up more than once:

```{r}
yes_why %>% 
  count(word) %>% 
  filter(n >1) %>% 
  ggplot(aes(x = reorder(word,n), y = n))+ #reorder makes the bars go in order high to low
  geom_col()+
  theme(axis.text.x = element_text(angle = 45))
```

Now lets do the same for the "No" responses and compare:

```{r}
no_why <- no %>% 
  select(`Why or why not?`) %>% 
  unnest_tokens(output = word, #the new column name to put the text in
                input = `Why or why not?`)  %>%
  anti_join(stop_words)
  
```

Snapshot of the word summary:

```{r}
no_why %>% 
  count(word, sort = TRUE)
```

```{r}
no_why %>% 
  count(word) %>% 
  filter(n >1) %>% 
  ggplot(aes(x = reorder(word,n), y = n))+ #reorder makes the bars go in order high to low
  geom_col()+
  scale_y_continuous(expand = c(0,0))+
  theme(axis.text.x = element_text(angle = 45))
```

Let's compare the top 5 words in "Yes" vs. "No" by binding our dataframes and faceting:

```{r}
yes_summary <- yes_why %>% 
  count(word) %>% 
  # take the top 5
  top_n(5) %>% 
  # create a new variable we can facet by later
  mutate(answer = "Yes")


# do the same for No
no_summary <- no_why %>% 
  count(word) %>% 
  # take the top 5
  top_n(5) %>% 
  # create a new variable we can facet by later
  mutate(answer = "No")
  
```

Now bind these into one dataframe and compare the answers

```{r}
bind_rows(yes_summary, no_summary) %>% 
  ggplot(aes(x = reorder(word,n), y = n))+
  geom_col()+
  facet_wrap(~answer)+
  theme(axis.text.x = element_text(angle = 45))
```

Another way to compare the answers is to calculate the proportion of each word in the dataset and create a correlation plot

```{r}
bind_rows(mutate(yes_why, answer = "yes"),
          mutate(no_why, answer = "no"))%>% 
  group_by(answer) %>% 
  count(word) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = answer, values_from = proportion) %>% 
  ggplot(aes(x = no, y = yes))+
  geom_jitter()+
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1)
```

### What are the pros and cons of open science?

Next, let's analyze the responses describing the pros and cons to open science.

For this example let's compare the responses using n-grams, which looks at adjacent words instead of just single words, so we can detect common phrases and word associations. The process is similar as before, using the `unnest_tokens()` function, but this time we add the argument `token = "ngrams"`. We also specify `n` for how many consecutive words to examine, starting with an `n = 2` argument which is often called a 'bigram'.

Let's start analyzing the *pros* of open science:

```{r}
pros_bigrams <- data %>% 
  select(ID, text = `What do you think are the pros of open science?`) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

Examine the most common pairs of words:

```{r}
pros_bigrams %>% 
  count(bigram, sort = TRUE)
```

Lets remove any stop words

```{r}
pros_bigrams %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)
```

Lets do the same for the cons of open science

```{r}
cons_biograms <- data %>% 
  select(ID, text = `What do you think are the cons of open science?`) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
```

Now clean and summarize:

```{r}
cons_biograms %>% 
  separate(bigram, into = c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n>1)
```

### Contribute to Equity and Environmental Justice

Lastly, let's work with the survey question 'In what ways do you believe you can contribute to equity and environmental justice?'

```{r}
data %>% 
  select(ID, text = `In what ways do you believe you (in any aspect of your life or career) can contribute to equity and environmental justice?`) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 3) %>% 
  separate(bigram, into = c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word) %>% 
  filter(!word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)
```
