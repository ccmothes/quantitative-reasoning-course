[["index.html", "Quantitative Reasoning for Ecosystem Science 1 Introduction Set up Instructions Navigating this site", " Quantitative Reasoning for Ecosystem Science Caitlin C. Mothes, PhD 2023-03-07 1 Introduction This site hosts the lab curriculum for Colorado State University’s ESS 330 course: Quantitative Reasoning for Ecosystem Science Set up Instructions Before getting started with the first lesson, please follow the instructions on the R Setup to make sure you have all the necessary software installed on your computer. Navigating this site The table of contents on the left allows you to navigate to the lesson for each week of the course. Each lesson will walk you through working through the topic, analysis, etc. for that week with exercises at the end of the lesson that will be that week’s homework assignment. Homework will be submitted through Canvas and exercise questions (including any code, figures, etc.) must be submitted in Word or PDF format. The Intro to R lesson will walk through how to create R Markdown documents in R, which you will use to write your code/answers to the exercises and then render to either Word or PDF report which is what you will submit through Canvas. "],["setup-instructions.html", " 2 Setup Instructions 2.1 Install R and RStudio 2.2 Package Installation", " 2 Setup Instructions This tutorial walks you through all the required setup instructions to use R and RStudio for this course. If you have any issues, please email your TA or attend office hours for assistance. 2.1 Install R and RStudio R is an open source language and software environment for statistical analysis and graphics (plus so much more!). You must first download the R software (for free) here: https://www.r-project.org/. Click the download link based on your operating system (OS). Then, for Mac users, install the latest release based on your macOS. For Windows users, click the ‘install R for the first time’ link. Note: If you already have R installed, you must have at least version 4.0.0. or greater, but it is best to have the most recent version installed (4.2.2) RStudio is a (also free) R Integrated Development Environment (IDE) that provides a built-in editor and other advantages such as version control and project management. Once you have the R software installed on your computer, you can install RStudio Desktop here: https://posit.co/download/rstudio-desktop/. Under Step 2 click the download RStudio Desktop button. 2.2 Package Installation While the R software comes with many pre-loaded functions (referred to as ‘base R’ functions) to perform various operations in R, there are thousands of R packages that provide additional reusable R functions. In order to use these functions you need to first install the package to your local machine using the install.packages() function. Once a package is installed on your computer you don’t need to install it again (but you may have to update it). Anytime you want to use the package in a new R session you can load it with the library() function. We will be working in RStudio this entire course, so after you have installed both R and RStudio, open a new session of RStudio. You will learn more about the ins and outs of RStudio in lab, but for set up purposes you will just be running code in the Console. Normally you want to save the code you write, but since package installation is only needed once (unless you are working on a new machine or need to update any packages) you can execute this directly in the console. Run the following three lines of code (one at a time) in the console. You can click the copy button in the upper right corner when you hover over the code chunk, then paste that after the &gt; in the Console. Spelling is important! Install the tidyverse package. The Tidyverse is actually a collection of multiple R packages designed for data manipulation, exploration, and visualization that you are likely to use in every day data analysis. When you install the Tidyverse, it installs all of these packages, and you can later load all of them in your R session with library(tidyverse). Since you are installing multiple packages here this may take a little while. install.packages(&quot;tidyverse&quot;) Install the palmerpenguins package. This is a data package that installs a couple of spreadsheets you can load and work with in R. install.packages(&quot;palmerpenguins&quot;) Install the rmarkdown package. Later on in the course and for your final projects you will be working in and rendering R Markdown files and reports. R Markdown is a notebook style interface integrating text and code, allowing you to create fully reproducible documents and render them to various elegantly formatted static or dynamic outputs. You can learn more about R Markdown at their website, which has really informative lessons on their Getting Started page, and see the range of outputs you can create at their Gallery page. install.packages(&quot;rmarkdown&quot;) To see if you successfully installed all three packages, use the library() function to load the packages into your session. You should either see nothing printed to the console after running library(), or in the case of the tidyverse you may see some messages printed. As long as there are no error messages, you should be all set! If you do get any error messages, please email you TA or attend office hours for assistance. library(tidyverse) library(palmerpenguins) library(rmarkdown) "],["introduction-to-r-and-rstudio.html", " 3 Introduction to R and RStudio 3.1 Getting to know RStudio 3.2 R Projects 3.3 Write your first script 3.4 Exercises", " 3 Introduction to R and RStudio 3.1 Getting to know RStudio When you first open RStudio, it is split into 3 panels: The Console (left), where you can directly type and run code (by hitting Enter) The Environment/History pane (upper-right), where you can view the objects you currently have stored in your environment and a history of the code you’ve run The Files/Plots/Packages/Help pane (lower-right), where you can search for files, view and save your plots, view and manage what packages are loaded in your library and session, and get R help Image Credit: Software Carpentry To write and save code you use scripts. You can open a new script with File -&gt; New File -&gt; R Script or by clicking the icon with the green plus sign in the upper left corner under ‘File’. When you open a script, RStudio then opens a fourth ‘Source’ panel in the upper-left to write and save your code. You can also send code from a script directly to the console to execute it by highlighting the entire code line/chunk (or place your cursor at the very end of the code chunk) and hit CTRL+ENTER on a PC or CMD+ENTER on a Mac. Image Credit: Software Carpentry It is good practice to add comments/notes throughout your scripts to document what the code is doing. To do this start your text with a #. R knows to ignore everything after a #, so you can write whatever you want there. Note that R reads line by line, so if you want your comments to carry over multiple lines you need a # at every line. 3.2 R Projects As a first step whenever you start a new project, workflow, analysis, etc., it is good practice to set up an R project. R Projects are RStudio’s way of bundling together all your files for a specific project, such as data, scripts, results, figures. Your project directory also becomes your working directory, so everything is self-contained and easily portable. We recommend using a single R Project for this course, so lets create one now. You can start an R project in an existing directory or in a new one. To create a project go to File -&gt; New Project: Let’s choose ‘New Directory’ then ‘New Project’. Now choose a directory name, this will be both the folder name and the project name, so use proper spelling conventions (no spaces!). We recommend naming it something course specific, like ‘ESS-330-2023’. Choose where on your local file system you want to save this new folder/project, then click ‘Create Project’. Now you can see your RStudio session is working in the R project you just created. You can see the working directory printed at the top of your console is now the project directory, and in the ‘Files’ tab in RStudio you can see there is an .Rproj file with the same name as the R project, which will open up this R project in RStudio whenever you come back to it. Test out how this .Rproj file works. Close out of your R session, navigate to the project folder on your computer, and double-click the .Rproj file. What is a working directory? A working directory is the default file path to a specific file location on your computer to read files from or save files to. Since everyone’s computer is unique, everyone’s full file paths will be different. This is an advantage of working in R Projects, you can use relative file paths, since the working directory defaults to wherever the .RProj file is saved on your computer you don’t need to specify the full unique path to read and write files within the project directory. 3.3 Write your first script Let’s start coding! The first thing you do in a fresh R session and at the beginning of a workfow is set up your environment, which mostly includes installing and loading necessary libraries and reading in required data sets. Let’s open a fresh R script and save it in our root (project) directory. Call this script something like ‘r-intro.R’. 3.3.1 Commenting code It is best practice to add comments throughout your code noting what you are doing at each step. This is helpful for both future you (say you forgot what a chunk of code is doing after returning to it months later) and for others you may share your code with. To comment out code you use a #. You can use as many #’s as you want, any thing you write on that line after at least one # will be read as a comment and R will know to ignore that and not try to execute it as code. At the top of your script, write some details about the script like a title, your name and date. # Introduction to R and RStudio # your name # date Now for the rest of this lesson, write all the code in this script you just created. You can execute code from a script (i.e., send it from the script to the console) in various ways (see below). Think of these scripts as your code notes, you can write and execute code, add notes throughout, and then save it and come back to it whenever you want. 3.3.2 Executing code Almost always you will start a script by installing and/or loading all the libraries/packages you need for that workflow. Add the following lines of code to your script to import our R packages you should have already installed from the R Setup page. #load necessary libraries library(tidyverse) library(palmerpenguins) To execute code you can either highlight the entire line(s) of code you want to run and click the ‘Run’ button in the upper-right of the Source pane or use the keyboard shortcut CTRL+Enter on Windows or CMD+Enter on Macs. You can also place your cursor at the very end of the line or chunk of code you want to run and hit CTRL+Enter or CMD+Enter. All functions and other code chunks must properly close all parentheses or brackets to execute. If you have an un-closed parentheses/bracket you will get stuck in a never ending loop and will keep seeing + printed in the console. To get out of this loop you can either close the parentheses or bracket, or hit ESC to start over. You want to make sure you see the &gt; in the console and not the + to execute code. 3.3.3 Getting help with code Take advantage of the ‘Help’ pane in RStudio! There you can search packages and specific functions to see all the available and required arguments, along with the default argument settings and some code examples. You can also execute a help search from the console with a ?. For example: ?mean 3.3.4 Functions R has many built in functions to perform various tasks. To run these functions you type the function name followed by parentheses. Within the parentheses you put in your specific arguments needed to run the function. Practice running these various functions and see what output is printed in the console. # mathematical functions with numbers log(10) # average a range of numbers mean(1:5) # nested functions for a string of numbers, using the concatenate function &#39;c&#39; mean(c(1,2,3,4,5)) # functions with characters print(&quot;Hello World&quot;) paste(&quot;Hello&quot;, &quot;World&quot;, sep = &quot;-&quot;) 3.3.4.1 Assignment operator Notice that when you ran the following functions above, the answers were printed to the console, but were not saved anywhere (i.e., you don’t see them stored as variables in your environment). To save the output of some operation to your environment you use the assignment operator &lt;-. For example, run the following line of code: x &lt;- log(10) And now you see the output, ‘2.3025…’ saved as a variable named ‘x’. You can now use this value in other functions x + 5 3.3.4.2 Base R vs. The Tidyverse You may hear the terms ‘Base R’ and ‘Tidyverse’ a lot throughout this course. Base R includes functions that are installed with the R software and do not require the installation of additional packages to use them. The Tidyverse is a collection of R packages designed for data manipulation, exploration, and visualization that you are likely to use in every day data analysis, and all use the same design philosophy, grammar, and data structures. When you install the Tidyverse, it installs all of these packages, and you can then load all of them in your R session with library(tidyverse). Base R and Tidyverse have many similar functions, but many prefer the style, efficiency and functionality of the Tidyverse packages, and we will mostly be sticking to Tidyverse functions for this course. 3.3.5 Data Types For this intro lesson, we are going to use the Palmer Penguins data set (which is loaded with the palmerpenguins package you installed during setup). This data was collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. Load the penguins data set. data(&quot;penguins&quot;) You now see it in the Environment pane. Print it to the console to see a snapshot of the data: penguins This data is structured is a data frame, probably the most common data type and one you are most familiar with. These are like the spreadsheets you worked with in the first lesson, tabular data organized by rows and columns. However we see at the top this is called a tibble which is just a fancy kind of data frame specific to the tidyvese. At the top we can see the data type of each column. There are five main data types: character: \"a\", \"swc\" numeric: 2, 15.5 integer: 2L (the L tells R to store this as an integer, otherwise integers can also be numeric data types) logical: TRUE, FALSE complex: 1+4i (complex numbers with real and imaginary parts) Data types are combined to form data structures. R’s basic data structures include atomic vector list matrix data frame factors You can see the data type or structure of an object using the class() function, and get more specific details using the str() function. (Note that ‘tbl’ stands for tibble). class(penguins) str(penguins) class(penguins$species) str(penguins$species) When we pull one column from a data frame, like we just did above with the $ operator, that returns a vector. Vectors are 1-dimensional, and must contain data of a single data type (i.e., you cannot have a vector of both numbers and characters). If you want a 1-dimensional object that holds mixed data types and structures, that would be a list. You can put together pretty much anything in a list. myList &lt;- list(&quot;apple&quot;, 1993, FALSE, penguins) str(myList) You can even nest lists within lists list(myList, list(&quot;more stuff here&quot;, list(&quot;and more&quot;))) You can use the names() function to retrieve or assign names to list and vector elements names(myList) &lt;- c(&quot;fruit&quot;, &quot;year&quot;, &quot;logic&quot;, &quot;data&quot;) names(myList) 3.3.6 Indexing Indexing is an extremely important aspect to data exploration and manipulation. In fact you already started indexing when we looked at the data type of individual columns with penguins$species. How you index is dependent on the data structure. Index lists: # for lists we use double brackes [[]] myList[[1]] myList[[&quot;data&quot;]] Index vectors: # for vectors we use single brackets [] myVector &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) myVector[2] Index data frames: # dataframe[row(s), columns()] penguins[1:5, 2] penguins[1:5, &quot;island&quot;] penguins[1, 1:5] penguins[1:5, c(&quot;species&quot;,&quot;sex&quot;)] penguins[penguins$sex==&#39;female&#39;,] # $ for a single column penguins$species 3.3.7 Read and Write Data We used an R data package today to read in our data frame, but that probably isn’t how you will normally read in your data. There are many ways to read and write data in R. To read in .csv files, you can use read_csv() which is included in the Tidyverse with the readr package, and to save csv files use write_csv(). The readxl package is great for reading in excel files, however it is not included in the Tidyverse and will need to be loaded separately. Find the .csv you created at the end of last week’s lesson. Created a folder in your project directory called ‘data/’ and copy the .csv file there. You will store various data sets in this ‘data/’ folder throughout the course. Say your .csv is called ‘survey_data_clean.csv’. This is how you would read it in to R, giving it the object name ‘survey_data’ survey_data &lt;- read_csv(&quot;data/survey_data_clean.csv&quot;) 3.4 Exercises For each question you must also include the line(s) of code you used to arrive at the answer. Use class() and str() to inspect the .csv you created from last week’s lesson that you just read into R. How many rows and columns does it have? What data type is each column? (4 pts) Why don’t the following lines of code work? Tweak each one so the code runs (6 pts) myList[year] penguins$flipper_lenght_mm penguins[island==&#39;Dream&#39;,] How many species are in the penguins dataset? What islands were the data collected for? (Note: the unique() function might help) (5 pts) Use indexing to create a new data frame that has only 3 columns: species, island and flipper length columns, and subset all rows for just the ‘Dream’ island. (5 pts) Use indexing and the mean() function to find the average flipper length for the Adelie species on Dream island. (5 pts) "],["exploratory-data-analysis.html", " 4 Exploratory Data Analysis 4.1 Data wrangling 4.2 Visualization", " 4 Exploratory Data Analysis For this lesson you will be working with the same penguins data from last week. Start a new script for this week, and add/execute the following lines of code to set up your session: library(tidyverse) library(palmerpenguins) data(&quot;penguins&quot;) When working with a new data set, often the first thing you do is perform some initial investigations on the data using various summary statistics and graphical representations. This is exploratory data analysis! Or for short, EDA. EDA is used to catch any potential errors, assess statistical assumptions, observe patterns and help form initial hypotheses of your data that you can then test with statistics. For our penguins data, we want to start by exploring things like sample size, variation and distribution of our variables, and make initital comparisons among species, islands, and sex. A new Base R function we have yet to use is summary(). This functions gives us a very quick snapshot of each variable in our dataset, where we can see things like sample size and summary statistics. summary(penguins) For some more in depth EDA, the tidyverse packages provide many useful functions to summarize and visualize data. Today we are going to simultaneously learn about various functions of tidyverse packages while investigating and formulating hypotheses about our penguins data set. 4.1 Data wrangling 4.1.1 The dplyr package dplyr is a Tidyverse package to handle most of your data exploration and manipulation tasks. Now that you have learned indexing in the Intro to R lesson, you may notice the first two dplyr functions we are going to learn, filter() and select() act as indexing functions, subsetting rows and columns based on specified names and/or conditions. Subset rows with filter() You can filter data in many ways using logical operators (&gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal)), AND (&amp;), OR (|), and NOT (!) operators, and other operations such as %in%, which returns everything that matches at least one of the values in a given vector, and is.na() and !is.na() to return all missing or all non-missing data. # filter rows for just the Adelie species filter(penguins, species == &quot;Adelie&quot;) # filter rows for all species EXCEPT Adelie filter(penguins, species != &quot;Adelie&quot;) # filter islands Dream and Torgersen AND rows that DO NOT have missing values for bill length filter(penguins, island %in% c(&quot;Dream&quot;, &quot;Torgersen&quot;) &amp; !is.na(bill_length_mm)) Note: Tidyverse package functions take in column names without quotations. Using dplyr functions will not manipulate the original data, so if you want to save the returned object you need to assign it to a new variable. body_mass_filtered &lt;- filter(penguins, body_mass_g &gt; 4750 | body_mass_g &lt; 3550) Subset columns with select() select() has many helper functions you can use with it, such as starts_with(), ends_with(), contains() and many more that are very useful when dealing with large data sets. See ?select for more details # Select two specific variables select(penguins, species, sex) # Select a range of variables select(penguins, species:flipper_length_mm) # Rename columns within select select(penguins, genus = species, island) # Select column variables that have &#39;mm&#39; in their name select(penguins, contains(&quot;mm&quot;)) Create new variables with mutate() mutate() allows you to edit existing columns or create new columns in an existing data frame, and you can perform calculations on existing columns to return outputs in the new column. The syntax is the name of the new column you want to make (or the current column you want to edit) on the left of =, and then to the right is what you want to put in the new column. Note that mutate() works row wise on the data frame. # New variable that calculates bill length in cm mutate(penguins, bill_length_cm = bill_length_mm/10) # mutate based on conditional statements with if_else() mutate(penguins, species_sex = if_else(sex == &#39;male&#39;, paste0(species,&quot;_m&quot;), paste0(species, &quot;_f&quot;))) Notice the use of paste0() here, and when we briefly used a similar function paste() in previous lessons. Explore the difference between these two. They are both very useful functions for pasting strings together. group_by() and summarise() All the above functions can all be used in conjunction with group_by(), which changes the scope of each function from operating on the entire data set to operating on it group-by-group. group_by() becomes even more powerful when used along with summarise() to calculate some specified summary statistic for each group. However, before we start using multiple operations in conjunction with one another, we need to talk about the pipe operator %&gt;%. 4.1.1.1 The pipe %&gt;% The pipe, %&gt;%, comes from the magrittr package by Stefan Milton Bache. Packages in the tidyverse load %&gt;% for you automatically, so you don’t usually load magrittr explicitly. Pipes are a powerful tool for clearly expressing a sequence of multiple operations. For example, the pipe operator can take this sequence of operations: df1 &lt;- filter(penguins, island == &quot;Dream&quot;) df2 &lt;- mutate(df1, flipper_length_cm = flipper_length_mm/10) df3 &lt;- select(df2, species, year, flipper_length_cm) print(df3) And turn it into this, removing the need to create intermediate variables penguins %&gt;% filter(island == &quot;Dream&quot;) %&gt;% mutate(flipper_length_cm = flipper_length_mm/10) %&gt;% select(species, year, flipper_length_cm) You can read it as a series of imperative statements: filter, then mutate, then select. A good way to pronounce %&gt;% when reading code is “then”. It takes the output of the operation to the left of %&gt;% and feeds it into the next function as the input. So now back to group_by() and summarize(). Say you want to summarize data by some specified group, for example you want to find the average body mass for each species, this is how you could do that: penguins %&gt;% group_by(species) %&gt;% summarise(body_mass_avg = mean(body_mass_g, na.rm = TRUE)) You can also group by multiple variables. Say you want to calculate the sample size (i.e., count, which can be calculated with the n() function) for each species for each year of the study. penguins %&gt;% group_by(species, year) %&gt;% summarise(n_observations = n()) 4.2 Visualization An important part of data exploration includes visualizing the data to reveal patterns you can’t necessarily see from viewing a data frame of numbers. Here we are going to walk through a very quick introduction to ggplot2, using some code examples from the palmerpenguins R package tutorial: https://allisonhorst.github.io/palmerpenguins/articles/intro.html. ggplot2 is perhaps the most popular data visualization package in the R language, and is also a part of the Tidyverse. One big difference about ggplot though is that it does not use the pipe %&gt;% operator like we just learned, but instead threads together arguments with + signs (but you can pipe a data frame into the first ggplot() argument). The general structure for ggplots follows the template below. Note that you can also specify the aes() parameters within ggplot() instead of your geom function, which you may see a lot of people do. The mappings include arguments such as the x and y variables from your data you want to use for the plot. The geom function is the type of plot you want to make, such as geom_point(), geom_bar(), etc, there are a lot to choose from. # general structure of ggplot functions ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) Visualize variable distributions with geom_historgram() If you plan on doing any statistical analysis on your data , one of the first things you are likely to do is explore the distribution of your variables. You can plot histograms with geom_histogram() ggplot(penguins) + geom_histogram(mapping = aes(x = flipper_length_mm)) This tells us there may be a lot of variation in flipper size among species. We can use the ‘fill =’ argument to color the bars by species, and scale_fill_manual() to specify the colors. # Histogram example: flipper length by species ggplot(penguins) + geom_histogram(aes(x = flipper_length_mm, fill = species), alpha = 0.5, position = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;darkorange&quot;,&quot;darkorchid&quot;,&quot;cyan4&quot;)) Cool, now we can see there seems to be some pretty clear variation in flipper size among species. Another way to visualize across groups is with facet_wrap(), which will create a separate plot for each group, in this case species. ggplot(penguins) + geom_histogram(aes(x = flipper_length_mm, fill = species), alpha = 0.5, position = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;darkorange&quot;,&quot;darkorchid&quot;,&quot;cyan4&quot;)) + facet_wrap(~species) Compare sample sizes with geom_bar() We saw a quick snapshot of total sample size for each species with summary(). Let’s use ggplot to see sample size for each species on each island. ggplot(penguins) + geom_bar(mapping = aes(x = island, fill = species)) As you may have already noticed, the beauty about ggplot2 is there are a million ways you can customize your plots. This example builds on our simple bar plot: ggplot(penguins, aes(x = island, fill = species)) + geom_bar(alpha = 0.8) + scale_fill_manual(values = c(&quot;darkorange&quot;,&quot;purple&quot;,&quot;cyan4&quot;), guide = FALSE) + theme_minimal() + facet_wrap(~species, ncol = 1) + coord_flip() This is important information, since we know now that not all species were sampled on every island, which will have complications for any comparisons we may want to make among islands. Visualize variable relationships with geom_point() We can use geom_point() to view the relationship between two continuous variables by specifying the x and y axes. Say we want to visualize the relationship between penguin body mass and flipper length and color the points by species: ggplot(penguins) + geom_point(mapping = aes(x = body_mass_g, y = flipper_length_mm, color = species)) 4.2.1 Exercises For each exercise, please include the line(s) of code you used to answer the question. Reorder the variables in penguins so that year is the first column followed by the rest (Hint: look into the use of everything()). (5 pts.) Create a new column called ‘size_group’ where individuals with body mass greater than the overall average are called ‘large’ and those smaller are called ‘small’. (Note: this answer requires the additional use of both the if_else() and mean() functions. Remember how to deal with NA values in mean()). (5 pts.) Which year had the largest average weight of all individuals according to body mass. (5 pts.) You want to filter data for years that are not in a vector of given years, but this code doesn’t work. Tweak it so that it does. (5 pts.) penguins %&gt;% filter(year !%in% c(2008, 2009)) Using the visualization techniques you learned today, create a figure that allows you to visualize some comparison of your choice among the penguins data set. Along with embedding the figure you made, write a testable hypothesis about the data and the patterns you see from this figure. (5 pts.) "],["introduction-to-statistics.html", " 5 Introduction to Statistics 5.1 Explore the dataset 5.2 Chi-square - Categorical Analysis 5.3 t-test - Compare two means 5.4 Correlation - Assess relationships 5.5 Exercises", " 5 Introduction to Statistics In this lesson you will be introduced to the process of conducting statistical tests in R, specifically chi-square, t-tests, and correlation tests. First, to access the dataset(s) you will be using today install the remotes package, and then install the lterdatasampler package (remotes is needed because lterdatasampler has to be installed from GitHub as opposed to CRAN). install.packages(&quot;remotes&quot;) remotes::install_github(&quot;lter/lterdatasampler&quot;) Now load in all libraries needed for this lesson: library(tidyverse) library(lterdatasampler) Then run the following line of code to retrieve the and_vertebrates data set and bring it into your R session: data(and_vertebrates) 5.1 Explore the dataset Do a little exploration of this data first to understand its structure, variables and data types: # View the data structure glimpse(and_vertebrates) # Explore the metadata in the Help pane ?and_vertebrates This data set contains length and weight observations for three aquatic species in clear cut and old growth coniferous forest sections of Mack Creek in HJ Andrews Experimental Forest in Oregon. The three species are Cutthroat trout, Coastal giant salamander and Cascade torrent salamander. 5.2 Chi-square - Categorical Analysis When you are working with two categorical variables, the statistical test you would use is a Chi-square test. This test can tell you if there is a relationship between your two categorical variables. For example, we have two categorical variables in the and_vertebrates data set: section = two forest sections, clear cut (CC) and old growth (OG) unittype = channel unit classification type (C = cascade, I = riffle, IP = isolated pool (not connected to channel), P = pool, R = rapid, S = step (small falls), SC = side channel, NA = not sampled by unit) Lets focus this question on Cutthroat trout. First explore the abundance of cutthroat trout in different channel types, using the n() function to return the total count/number of observations in each group. and_vertebrates %&gt;% filter(species == &quot;Cutthroat trout&quot;) %&gt;% group_by(unittype) %&gt;% summarise(abundance = n()) This output tells us that there are quite a few observations with the NA category, meaning channel type was unknown or not recroded. Let’s edit the workflow above slightly, using two new functions: drop_na() and count(). drop_na() will remove any rows within a specified column (or columns) that have NA values, and we can use count() as an alternative to group_by() and summarise() when we just want number of observations for a single variable (in this case unittype). and_vertebrates %&gt;% filter(species == &quot;Cutthroat trout&quot;) %&gt;% drop_na(unittype) %&gt;% count(unittype) This returns just about the same data frame as the first method, but now with the NA category removed because it dropped any observations that were NA for unittype. From this we also observe that the highest Cutthroat trout abundances are found in cascade (C), pool (P), and side channel (SC) habitats. Now, our question expands beyond this one categorical variable (channel type) and we want to know if abundance is affected by both channel and and forest type (section). Here, our null hypothesis is that forest and channel type are independent. To test this, we use the chisq.test() function to carry out a chi-square test, but first we have to reformat our data into a contingency table. A contingency table is in matrix format, where each cell is the frequency (in this case seen as abundance) of Cutthroat trout in each combination of categorical variables (forest type and channel unit). We can create a contingency table with the table() function. For this analysis, lets also filter out just the 3 most abundant unit types for Cutthroat trout (C, P and SC). # First clean the dataset to create the contingency table from trout_clean &lt;- and_vertebrates %&gt;% #filter Cutthroat trout filter(species == &quot;Cutthroat trout&quot;) %&gt;% # lets test using just the 3 most abundant unittypes filter(unittype %in% c(&quot;C&quot;, &quot;P&quot;, &quot;SC&quot;)) %&gt;% # drop NAs for both unittype and section drop_na(unittype, section) cont_table &lt;- table(trout_clean$section, trout_clean$unittype) To execute the Chi-square test does not take that much code, but it is important to note that by default, chisq.test() assumes the null hypothesis is that all frequencies have equal probability. If you have different pre-conceived frequency probabilities for your data you have to define those within the chisq.test() function. chisq.test(cont_table) Looking at these results, we have an extremely small p-value. This tells us that there is a significant relationship between forest type and channel unit (i.e., we rejected our null hypothesis). Lets look at the abundance distribution visually: trout_clean %&gt;% count(unittype, section) %&gt;% ggplot(aes(x = unittype, y = n))+ geom_col(aes(fill = section))+ scale_fill_manual(values = c(&quot;orange&quot;, &quot;darkgreen&quot;))+ theme_minimal() 5.3 t-test - Compare two means Previous work has shown that forest harvesting can impact aquatic vertebrate biomass (Kaylor &amp; Warren 2017). With this and_vertebrates data set we can investigate this hypothesis, by comparing weight to forest type (clear cut or old growth). This therefore involves a test comparing the means (average weight) among two groups (clear cut and old growth forests), which then requires a t-test. Lets focus on conducting this test for just Cutthroat trout (to reduce species-level variances in weight), so we can use the same trout_clean data set we made earlier, but let’s also drop all NAs in weight_g. Then, lets first visualize the differences in weight among forest type with a boxplot: trout_clean %&gt;% drop_na(weight_g) %&gt;% ggplot(aes(x = section, y = weight_g))+ geom_boxplot() We don’t see too much of a difference based on this visual, but lets conduct the statistical test to really verify if our hypothesis is supported. First however we need to check our test assumptions, which for t-tests assumes the variance of the groups is equal. We can test for equal variances with the function var.test(), where the null hypothesis is that the variances is equal. In this step we need two vectors of the weights in each separate forest section. You can use pull() to convert a single column of a data frame/tibble to a vector, and we want to do this for clear cut and old growth forests separately. We then put both of those vectors in the var.test() function to assess their equal variances. cc_weight &lt;- trout_clean %&gt;% filter(section == &quot;CC&quot;) %&gt;% pull(weight_g) og_weight &lt;- trout_clean %&gt;% filter(section == &quot;OG&quot;) %&gt;% pull(weight_g) var.test(cc_weight, og_weight) Looks like our variances are not equal. We have two options now, we can either transform our weight variable or use the Welch t-test which does not assume equal variances. Variable transformation If we look at the distribution of weight (our continuous variable), it is pretty right skewed. Therefore, we’d likely want to do a log transformation on the data, which works well the data is skewed like this: hist(trout_clean$weight_g) Lets perform the variances check like we did before, but on the log transformed values, which you can do with log var.test(log(cc_weight), log(og_weight)) Now we have a high p-value, indicating support for the null that the variances are equal. So. we can use the default t.test() test which assumes equal variances, but on a log transformed weight variable. The t.test() function in R takes in your dependent (in our case trout weight) and independent (forest type) variables as vectors (instead of just column names like you can do in the Tidyverse). Remember how we can index single columns of data frames with the $ operator. The order of the variables in the t.test() function is {dependent variable} ~ {independent variable}. We use the ~ to specify a model, telling the test we want to know if weight varies by forest section. Remember we also want to log transform the weight values and then specify that our variances are equal since we confirmed that with var.test() above, so the final t.test() call would be this: t.test(log(trout_clean$weight_g) ~ trout_clean$section, var.equal = TRUE) The output of this test gives us the test statistics, p-value, and the means for each of our forest groups. Given the p-value of 0.0043, we can conclude that we reject the null hypothesis that mean Cutthroat weight is the same in clear cut and old growth forest sections, and looking at our results (specifically the means) we can conclude that Cutthroat trout weight was observed to be significantly higher in clear cut forests compared to old growth forests. Remember though that now these mean weight values are log transformed, and not the raw weight in grams. The relationship can still be interpreted the same. How does this relate to your original hypothesis? Welch Two Sample t-test Alternatively, instead of transforming our variable we can actually change the default t.test() argument by specifying var.equal = FALSE, which will then conduct a Welch t-test, which does not assume equal variances among groups. t.test(trout_clean$weight_g ~ trout_clean$section, var.equal = FALSE) While we used a slightly different method, our conclusions are still the same, finding that Cutthroat trout had significantly higher weights in clear cut forests than old growth. Note: In the t.test() function you can add paired = TRUE to conduct a paired t-test. These are for cases when the groups are ‘paired’ for each observation, meaning each group/treatment was applied to the same individual, such as before and after experiments. 5.4 Correlation - Assess relationships When you want to assess the relationship between two continuous variables, the test you would use is a correlation test. Correlation tests asses both the presence of a significant relationship along with the strength of that relationship (i.e., the correlation coefficient). For our and_vertebrates data set, we can test length-mass relationships for our species with our length and weight continuous variables. Lets test the hypothesis that body length is positively correlated with weight, such that longer individuals will also weigh more, specifically looking at the Coastal Giant salamander. First let’s clean our data set to just include the Coastal giant salamander and remove missing values for length and weight. Let’s focus on the variable ‘length_2_mm’ for snout to tail length. sally_clean &lt;- and_vertebrates %&gt;% filter(species == &quot;Coastal giant salamander&quot;) %&gt;% drop_na(length_2_mm, weight_g) Now we can perform the correlation test with the cor.test() function. There are multiple correlation methods you can use with this function, by default it uses the Pearson correlation method. However this test assumes that your data is normally distributed and there is a linear relationship, so if that is not the case you can specify spearman for method = to use a Spearman Rank correlation test, a non-parametric test that is not sensitive to the variable distribution. Let’s look at the distribution of these variables first: hist(sally_clean$length_2_mm) hist(sally_clean$weight_g) They both look pretty skewed, therefore likely not normally distributed. We can statistically test if a variable fits a normal distribution with the shapiro.test() function. However note that this function only runs for 5000 observations or less, so lets test for normally of the first 5000 obs of our sally_clean data set: shapiro.test(sally_clean$length_2_mm[1:5000]) shapiro.test(sally_clean$weight_g[1:5000]) The null hypothesis of the Shapiro-Wilk normality test is that the variable is normally distributed, so a significant p-value less than 0.05 (as we see for both of our variables here) tells use that our data does not fit a normal distribution. Therefore we have two options as we did with our t-test example: transform the variables or use the non-parametric test. Variable transformation Lets try the first option by log transforming our variables (since we saw they both had pretty skewed distributions), first viewing the new distribution and then performing the Pearson’s correlation test (the default for cor.test()). hist(log(sally_clean$length_2_mm)) hist(log(sally_clean$weight_g)) All we need to add to the cor.test() argument is the two variables of our sally_clean data set we want to test a relationship for, and let’s keep them log-transformed since those distributions looked closer to a normal distribution (visually at least). cor.test(log(sally_clean$length_2_mm), log(sally_clean$weight_g)) Okay, from these results we see a very small p-value, meaning there is a significant association between the two, and a correlation coefficient of 0.98, representing a very strong, positive correlation. Let’s look at this correlation visually: sally_clean %&gt;% ggplot(aes(x = log(length_2_mm), y = log(weight_g)))+ geom_point() We can use geom_smooth() to add a line of best fit using a linear model equation (which you will learn more about next week) sally_clean %&gt;% ggplot(aes(x = log(length_2_mm), y = log(weight_g)))+ geom_point()+ geom_smooth(method = &quot;lm&quot;) Spearman Correlation Test Let’s now perform the correlation test again but keeping our raw data and instead specifying method = 'spearman', as the Spearman test is better for non-parametric and non-linear data sets. cor.test(sally_clean$length_2_mm, sally_clean$weight_g, method = &quot;spearman&quot;) These results also represent a significant, positive relationship between length and weight for the Coastal Giant salamander, with a very high correlation coefficient. 5.5 Exercises Each question requires you to carry out a statistical analysis to test some hypothesis related to the and_vertebrates data set. To answer each question fully: Include the code you used to clean the data and conduct the appropriate statistical test. (Including the steps to assess and address your statistical test assumptions). Report the findings of your test in proper scientific format (with the p-value in parentheses). 1. Conduct a chi-square test similar to the one we carried out earlier in this lesson plan, but test for a relationship between forest type (section) and channel unit (unittype) for Coastal giant salamander abundance. Keep all unittypes instead of filtering any like we did for the Cutthroat trout (9 pts.) 2. Test the hypothesis that there is a significant difference in species biomass between clear cut and old growth forest types for the Coastal Giant salamander. (8 pts.) 3. Test the correlation between body length (snout to fork length) and body mass for Cutthroat trout. (Hint: run ?and_vertebrates to find which length variable represents snout to fork length) (8 pts.) 5.5.1 Acknowledgements Thanks to the developers of lterdatasampler for providing the data set and vignettes that helped guide the creation of this lesson plan. 5.5.2 Citations Data Source: Gregory, S.V. and I. Arismendi. 2020. Aquatic Vertebrate Population Study in Mack Creek, Andrews Experimental Forest, 1987 to present ver 14. Environmental Data Initiative. https://doi.org/10.6073/pasta/7c78d662e847cdbe33584add8f809165 Kaylor, M.J. and D.R. Warren. 2017. Linking riparian shade and the legacies of forest management to fish and vertebrate biomass in forested streams. Ecosphere 8(6). https://doi.org/10.1002/ecs2.1845 "],["multivariate-statistics.html", " 6 Multivariate Statistics 6.1 Explore the Data set 6.2 ANOVA 6.3 Simple Linear Regression 6.4 Multiple Linear Regression 6.5 Exercises", " 6 Multivariate Statistics In this lesson you will be introduced to statistical tests for dealing with more complex data sets, such as when you need to compare across more than two groups (ANOVA) or assess relationships in the form of an equation to predict response variables given single or multiple predictors (Regression). First you’ll need to load in the libraries and data set for the lesson. We need to install one new package for today to use a specific statistical test. This package is called car. Follow the steps below to install the package, and then read in your libraries and data set for the lesson. #install the car package install.packages(&quot;car&quot;) #load in packages library(tidyverse) library(lterdatasampler) library(car) # data set data(&quot;pie_crab&quot;) 6.1 Explore the Data set This data set consists of Fiddler crab body size measured in salt marshes from Florida to Massachusetts during summer 2016 at Plum Island Ecosystem LTER. glimpse(pie_crab) Learn more about each variable: ?pie_crab This data set provides a great opportunity to explore Bergmann’s rule: where organisms at higher latitudes are larger than those at lower latitudes. There are various hypotheses on what drives this phenomenon, which you can read more about in Johnson et al. 2019. We have a continuous size variable (carapace width in mm), our dependent variable, and various predictor variables: site (categorical), latitude (continuous), air temperature (continuous) and water temperature (continuous). Let’s explore the sample size at each site and how many sites are in this data set # sample size per site pie_crab %&gt;% group_by(site) %&gt;% count() We have 13 sites with ~30 individual male crabs measured at each site. Let’s also check the range of our continuous variables: summary(pie_crab) 6.2 ANOVA First we can see if there is a significant difference in crab size among sites. Since we have a continuous response variable (size) and a categorical predictor (site) with &gt; 2 groups (13 sites), we will use an ANOVA test. Lets first visualize the distribution of size values for each site using a new visualization technique with ggplot called geom_jitter(). This function adds a small amount of variation to each point, so that all our points for each site are not stacked on top of each other (for example, try running the following code below but with geom_point() instead of geom_jitter() and notice the difference). In this code we also use the reorder() function to order our x axis value (site) by latitude to see any initial trends fitting Bergmann’s rule. pie_crab %&gt;% ggplot(aes(x = reorder(site, latitude), y = size, color = site)) + geom_jitter()+ # edit y axis label labs(x = &quot;&quot;, y = &quot;Carapace width (mm)&quot;)+ # remove the legend and x axis label theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()) Looks like there is variation among sites, so lets test for statistical significance with the ANOVA test. 6.2.1 Assumptions Normality ANOVA assumes normal distributions within each group. Here our group sample sizes are ~30 each which can be considered as large enough to not worry about this assumption, but lets walk through how to statistically check for normality if you had smaller sample sizes. You could test for normality with the Shaprio-Wilk test for each group individually, but here we have a lot of groups (13) and that would be tedious. Instead, we can calculate the residuals for all groups and test for normal distribution on the single set of residuals. A residual value is computed for each observation as the difference between that value and the mean of all values for that group. We can get the residuals from the ANOVA model by running aov(). To carry out the ANOVA model, we specify the name of our continuous response (size) ~ the name of our categorical predictor (site), and specify the data set name. Note that the aov() function won’t work the %&gt;% pipe. res_aov &lt;- aov(size ~ site, data = pie_crab) We can then pull out the residuals of this aov() model like we do by indexing columns with the $ operator. Let’s check the distribution visually with hist() and then statistically with shapiro.test(). hist(res_aov$residuals) shapiro.test(res_aov$residuals) This returns a p-value of 0.72, so we accept the null that this data does fit the normal distribution assumption. Equal Variances To test for equal variances among more than two groups, it is easiest to use a Levene’s Test. To use this test we need to install a new package called car, which you should have done at the beginning of this lesson. leveneTest(size ~ site, data = pie_crab) Similar to the var.test() function you’ve used before, the null hypothesis of the Levene’s test is that the variances are equal. Given this small p-value (denoted the the ‘Pr(&gt;F)’ value) we see that the variances of our groups are not equal. Therefore we would have to perform a Welch ANOVA: oneway.test(size ~ site, data = pie_crab, var.equal = FALSE) Our results here are highly significant, meaning that at least one of our groups means is significantly different from the others. Now ANOVAs don’t tell us which groups are significantly different, for that we would need to use the post-hoc Tukey’s HSD test. However for 13 groups that is a lot of pairwise comparisons to perform. For the next example lets filter our analysis to check for differences among 3 sites, choosing sites at the two extremes in latitude and one in the middle of the range. pie_sites &lt;- pie_crab %&gt;% filter(site %in% c(&quot;GTM&quot;, &quot;DB&quot;, &quot;PIE&quot;)) We already know that this data set fits the normality assumption, but now lets check if the variances of these 3 sites are equal or not. leveneTest(size ~ site, data = pie_sites) A p-value of 0.58 is much higher than our cut-off of 0.05, so we are confident that the variances are equal and we can therefore carry out the ANOVA with the aov() as we meet all its assumptions. pie_anova &lt;- aov(size ~ site, data = pie_sites) To view the ANOVA results of this model we use summary() summary(pie_anova) 6.2.2 Post-hoc Tukey’s HSD test From the ANOVA test we find that at least one of our group means is significantly different from the others. Now we can use the TukeyHSD() function to test all the pairwise differences to see which groups are different from each other. TukeyHSD(pie_anova) This returns each combination of site comparisons and a p-value (the ‘p adj’ variable) for each. 6.3 Simple Linear Regression Lets more directly test Bergmann’s rule by testing for a relationship between carapace width and latitude. Since our predictor (latitude) is a continuous, quantitative variable, we can conduct a simple linear regression. To conduct a regression model, we use the lm() function. pie_lm &lt;- lm(size ~ latitude, data = pie_crab) #view the results of the linear model summary(pie_lm) Our p-value is indicated in the ‘Pr(&gt;|t|)’ column for ‘latitude’ and at the bottom of these results, telling us that latitude does have a significant effect on crab size. From the results we also have an Estimate for latitude (0.49), which reflects the regression coefficient or strength and direction of the effect of latitude, along with the standard error for that estimate (0.03), reflecting the variation in that estimate. Lets view this visually and fit the linear regression line of best fit. pie_crab %&gt;% ggplot(aes(x = latitude, y = size))+ geom_point()+ geom_smooth(method = &quot;lm&quot;) Now that we fit this model, we can use it to predict crab size at different latitudes with predict(). For example, lets predict carapace width at a latitudes of 32, 36, and 38 degrees. Note that we need to create these values as a new data frame with the same column name used in the data that the model was built of off. new_lat &lt;- data.frame(latitude = c(32, 36, 38)) predict(pie_lm, newdata = new_lat) 6.4 Multiple Linear Regression Say we want to model the effect of more than one predictor on crab size. In this data set we also have continuous variables for air temperature and water temperature. Lets model the effect of latitude, air and water temperature on carapace width. Running a multiple linear regression is very similar to the simple linear regression, but now we specify our multiple predictor variables by adding them together with a + sign like this: pie_mlm &lt;- lm(size ~ latitude + air_temp + water_temp, data = pie_crab) summary(pie_mlm) These results show an overall p-value for the model, indicating a significant impact of the combination of predictor variables on crab size, and individual p-values for the effect of each individual predictor on crab size. Note however that normally with multiple regression, one of the assumptions is that there is no correlation between the predictor variables. We can test for correlations between more than two variables with the cor() function. Lets test for correlation between our three predictors: pie_crab %&gt;% select(latitude, air_temp, water_temp) %&gt;% cor() Normally tests remove variables that have a correlation coefficient greater than 0.7/-0.7. These are all highly correlated (with coefficients near 1/-1), therefore probably not the best set of predictors to use for a multiple linear regression. Below in your assignment you will perform a multiple linear regression using variables that are a bit less correlated. 6.5 Exercises After completing the ANOVA test (and post-hoc Tukey’s HSD) in section 6.2 to test for significant differences in crab size among 3 different sites: 1) Create a boxplot showing the carapace width for each site where sites are ordered by latitude and 2) report the findings of the statistical test as you would in a scientific paper. Include both the code to create the boxplot and an image of the figure. (6 pts.) Conduct a simple linear regression for the effect of water_temp_sd (a measure reflecting annual variation in water temperature) on carapace width. Report your findings (include code and a sentence reporting the results) AND create a plot with a line of best fit. Include both the code to create the plot and an image of the figure. (10 pts). Conduct a multiple linear regression for the effects of latitude, air_temp_sd, and water_temp_sd on carapace width. First check for correlations among the three predictor variables (and report the correlation table) and second report your findings from the multiple linear regression (code and a sentence reporting the results). (9 pts.) 6.5.1 Acknowledgements Thanks to the developers of lterdatasampler for providing the data set and vignettes that helped guide the creation of this lesson plan. 6.5.2 Citations Johnson, D. 2019. Fiddler crab body size in salt marshes from Florida to Massachusetts, USA at PIE and VCR LTER and NOAA NERR sites during summer 2016. ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/4c27d2e778d3325d3830a5142e3839bb (Accessed 2021-05-27). Johnson DS, Crowley C, Longmire K, Nelson J, Williams B, Wittyngham S. The fiddler crab, Minuca pugnax, follows Bergmann’s rule. Ecol Evol. 2019;00:1–9. https://doi.org/10.1002/ece3.5883 "],["pca-and-r-markdown-intro.html", " 7 PCA and R Markdown Intro 7.1 Intro to R Markdown 7.2 Principal Component Analysis (PCA) 7.3 Variance explained 7.4 Variable loadings 7.5 Visualize patterns 7.6 A Note on Rendering: 7.7 Exercises", " 7 PCA and R Markdown Intro Before we get started with conducing a Principal Component Analysis (PCA), we are going to carry out this lesson in an R Markdown document and see how it can be used to 1) organize workflows with text, code, and figures/results and 2) render a nicely formatted report, which is what you will submit for this week’s assignment. 7.1 Intro to R Markdown R Markdown is a notebook style interface integrating text and code, allowing you to create fully reproducible documents and render them to various elegantly formatted static or dynamic outputs. You can learn more about R Markdown at their website, which has really informative lessons on their Getting Started page and see the range of outputs you can create at their Gallery page. 7.1.1 Getting started with R Markdown Let’s create a new document by going to File -&gt; New File -&gt; R Markdown. You will be prompted to add information like title and author. Give it the title “Week 7 Assignment- PCA” and change the output to Word document. Click OK to create the document. This creates an outline of an R Markdown document, and you see the title, author and date you gave the prompt at the top of the document which is called the YAML header. Notice that the file contains three types of content: An (optional) YAML header surrounded by ---s R code chunks surrounded by ```s text mixed with simple text formatting Since this is a notebook style document, you run the code chunks by clicking the green play button, and then the output is returned either directly below the chunk or in the console depending on settings in Tools -&gt; Global Options -&gt; R Markdown. When you want to create a report from your notebook, you render it by hitting the ‘Knit’ button in the top of the Source pane, and it will render to the format you have specified in the YAML header. In order to do so though, you need to have the rmarkdown package installed (which you will do below). You can delete the rest of the code/text below the YAML header, and insert a new code chunk at the top. You can insert code chunks by clicking the green C with the ‘+’ sign at the top of the source editor, or with the keyboard short cut (Ctrl+Alt+I for Windows, Option+Command+I for Macs). For the rest of the lesson you will be writing and executing code through code chunks, and you can type any notes in the main body of the document. The first chunk is almost always your set up code, where you read in libraries and any necessary data sets. We need three new packages for this lesson. rmarkdown is necessary in order to render your R Markdown (or .Rmd) files to specified formats (e.g., HTML, Word, PDF). plotly is a graphics library that we will use to demonstrate how you can make ggplot figures interactive and ggfortify is needed to plot PCA objects with ggplot2. Since you only need to install packages once, you do not need to include this in your R Markdown document, you can just run it directly in the console: install.packages(&quot;rmarkdown&quot;) install.packages(&quot;plotly&quot;) install.packages(&quot;ggfortify&quot;) This chunk should be placed at the beginning of your document to set up your environment for carrying out the analysis for the lesson. library(tidyverse) library(lterdatasampler) library(plotly) library(ggfortify) # retrieve data data(&quot;hbr_maples&quot;) The hbr_maples data set consists of sugar maple seedling traits measured in calcium-treated and non-treated sites to study the response of seedlings to calcium addition at the Hubbard Brook LTER. Let’s learn a little more about this data set: ?hbr_maples We have a lot of continuous variables representing leaf characteristics, and a few categorical variables (calcium-treated and non treated (reference) sites, low and mid elevation sites). With a large set of quantitative variables that are likely inter-correlated, this is a highly multivariate data space. This is where Principal Component Analysis (PCA) comes in to play. PCA is a type of ordination technique, which is a way of organizing things based on how similar they are. A PCA finds a new coordinate system by defining principal component (PC) axes that best account for the variation in this multivariate space, essentially reducing a large set of variables down to two variables that represent the two PC axes that (often) explain the most variance in the data. PCA is a common EDA (exploratory data analysis) technique to see patterns in multi-variable data sets, such as clusters among sites or samples, and/or you can use the PC variables in other analyses such as linear regression. 7.2 Principal Component Analysis (PCA) First, what is the temporal scale of this data set: unique(hbr_maples$year) Data was collected for two years, 2003 and 2004. Let’s run through this lesson with just the 2003 data. You notice that if you filter out just the 2004 samples, there was no data collected on leaf area, so lets just analyze the 2003 data which has 6 different trait variables. Its important to note that the PCA test does not handle NA values well, so let’s use drop_na() here to drop any additional observations that have may have NAs for the quantitative variables. Note: using the : operator will select the range of columns from that on the left of : to that on the right, which works well for this data set since all of our quantitative variables of interest are ordered together. maples03 &lt;- hbr_maples %&gt;% filter(year == 2003) %&gt;% drop_na(stem_length:corrected_leaf_area) Let’s first see if our quantitative variables are inter-correlated at all, as a PCA does not work the best with entirely uncorrelated data. To do this, we can use the cor() test you’ve seen before, and we need to reduce our maples_03 data to just the quantitative variables we are performing the PCA on. vars &lt;- maples03 %&gt;% select(stem_length:corrected_leaf_area) cor(vars) From this we see quite a few correlated variables (coefficients ~0.7 or greater). This tells us a PCA is a good analysis to use to summarize this data set. There are a few different functions to conduct a PCA in R (many available in additional R packages such as the vegan package for community analyses), but we are going to use the prcomp() function from base R, a reliable and stable function for conducting PCA. prcomp() takes in just the quantitative variables you want to perform the PCA on, so we can use the vars object we just created above. PCA is influenced by the magnitude of each variable, therefore scaling all your variables is often necessary. The prcomp() function can do this for us by adding scale = TRUE. maples03_pca &lt;- prcomp(vars, scale = TRUE) 7.3 Variance explained First we want to view the PC axes created and assess the variance explained by each. We can do this with summary() of our PCA object. summary(maples03_pca) We can also view these results visually using a plot called a screeplot. screeplot(maples03_pca) Looks like the first 2 axes explain nearly all the variance of the data space (&gt;80%, which is often the desired outcome). Notice that this function plots ‘Variances’ on the y-axis i.e., the Eigenvalues which reflect the total amount of variance explained by that axis instead of the proportion. The proportions are still interpretable from the size of the bar plots however. 7.4 Variable loadings We can next view the individual loadings of each variable (i.e., how much each variable contributes to each axis), by indexing the rotation element of the PCA object with the $ operator. maples03_pca$rotation Note that when we look at these individual loadings, when variables have opposite +/- values that means those variables are negatively correlated. From this we see that on the first axis PC1 (which explains the vast majority of the variance in the data set) all these variables are positively correlated. On PC2 we see a few negatively correlated, such as stem length and leaf dry mass. 7.5 Visualize patterns Now lets visualize some patterns with a biplot. We can create a biplot in base R with the biplot() function biplot(maples03_pca) This however is pretty messy and hard to interpret. With the ggfortify package we can create biplots with ggplot2 using the autoplot() function. We add loadings = TRUE and loadings.label = TRUE to add the variable loadings to the plots, on top of the sample scores (denoted with points). autoplot(maples03_pca, loadings = TRUE, loadings.label = TRUE) This is still a little messy. To better view and interact with this visualization, we can leverage interactive plotting with the plotly package, and make this ggplot object interactive by putting it inside the ggplotly() function: ggplotly(autoplot(maples03_pca, loadings = TRUE, loadings.label = TRUE)) Notice now you can hover over the vector lines and points to see the raw values, and also zoom in to the plot to see the names of the clustered variable loadings better. We can view more patterns in the data by coloring points (i.e., seedling samples) by one of the other variables in our data set. This project involved assessing the impacts of Calcium addition on sugar maple seedlings by comparing seedling traits among calcium treated (W1) and untreated (Reference) sites. Therefore, let’s color points by watershed treatment (the watershed variable) to see if there is any clustering in seedling traits among treatments. To do so with the autoplot() function we also specify the original dataset we used in prcomp() and the name of the column we want to color by. (Note the colour: argument is spelled the British way.) Let’s make this interactive as well by using ggplotly(). ggplotly(autoplot(maples03_pca, data = maples03, colour = &quot;watershed&quot;)) There appears to be some visual separation/clustering among watershed types along PC1 (the x-axis). This pattern may lead you to statistically test for differences among watershed treatments, which you will carry out in Exercise 3 below. 7.6 A Note on Rendering: When knitting/rendering an R Markdown document to Word or PDF (anything other than HTML), it will fail if there is any code executing interactive visualizations. This includes anything created with ggplotly(). Therefore, before rendering your assignment to a Word doc, you should add eval = FALSE to any code chunk using ggplotly(). Put this argument at the top of the code chunk. It should look like this: {r eval = FALSE}. 7.7 Exercises To complete the assignment this week, you will write your entire workflow (i.e., the entire PCA lesson above) in an R Markdown document. At the bottom of your R Markdown document (after carrying out the PCA) please add an ‘Exercises’ section, write (or paste) questions 1-3 below and write your responses directly below them, and finally knit your completed R Markdown file as a Word document. For the assignment you will only need to submit the rendered Word document, which should contain all the code you ran AND your responses to the questions below (some of which also include code!). Looking at the biplot, interpret the variable loadings based on vector angles. Which variable are positively, negatively, and uncorrelated with each other? (5 pts.) Make a biplot (using the autoplot function) and color samples by elevation. Include the static plot (i.e., do not use ggplotly). Do you notice any clustering? (8 pts.) We notice that there seems to be a visual separation among watershed treatment along our PC1 axis (which represents the entire set of seedling traits). Since we now have a single quantitative variable (PC1) and a categorical predictor (watershed), we can perform a t-test between watershed treatment to see if this difference in seedling traits is significant. Run the following chunk of code to add the PC1 variable to the original maples03 data. Then perform a t-test and report your findings. Include all the code you ran (remember how to test for t-test assumptions and properly address them if needed) and format your interpretation as you would in a scientific paper/report. (12 pts.) # the PCA operates row wise, so we can bind the columns since they have the same number of rows that are all paired up. Recall that the &#39;x&#39; element of the PCA shows each individual sample score on each axis maples03 &lt;- bind_cols(maples03, maples03_pca$x) "],["introduction-to-spatial-data-in-r.html", " 8 Introduction to Spatial Data in R 8.1 Set Up 8.2 Spatial Data Formats 8.3 Import and manipulate spatial data 8.4 Reading and Writing Spatial Data 8.5 Before Rendering! 8.6 Exercises", " 8 Introduction to Spatial Data in R 8.1 Set Up You will return this assignment similar to last week, by working through this lesson in an R Markdown document, answering the Exercises at the end, and knitting as a Word document to submit to Canvas. Therefore, begin this lesson by creating a a new R Markdown document, and make sure to select output as Word. 8.1.1 Package Installation To carry out this lesson, you will need to install a couple new R packages to import and work with spatial data. The two main packages for working with spatial data are sf (for vector data) and terra (for spatial data). We will also be using tmap to visualize spatial data and make quick maps, along with the tigris package to import some vector data. Run the following chunk of code in your console, comment it out, OR add eval = FALSE in the top of the code chunk. You do not want it to be included when you knit the R Markdown document, because it re-install the packages every time you knit. install.packages(&quot;sf&quot;) install.packages(&quot;terra&quot;) install.packages(&quot;tmap&quot;) install.packages(&quot;tigris&quot;) Now we need to read in these packages at the beginning of our workflow. You should have this as an executable code chunk in your R Markdown document. library(tidyverse) library(sf) library(terra) library(tmap) library(tigris) 8.1.2 Data download Second, you will need to download an elevation raster file to carry out this lesson. If you haven’t already, in the R Project you have been using in this class, create a data/ folder. Then, click the download button below, and save the file (elevation.tif) in the data/ folder. Download elevation raster 8.2 Spatial Data Formats Vector Data Locations (points) Coordinates, address, country, city Shapes (lines or polygons) Political boundaries, roads, building footprints, water bodies Raster Data Images (matrix of cells organized by rows and columns) Satellite imagery, climate, landcover, elevation 8.3 Import and manipulate spatial data 8.3.1 Vector Data tigris Polygons All the data we are working with in this lesson is confined to the state of Colorado. Let’s start by pulling in political boundaries for Colorado counties with the tigris package, which returns a shapefile consisting of polygons for each county. # download county shapefile for the state of Colorado counties &lt;- counties(state = &quot;CO&quot;) The tigris package is one of many data retrieval R packages that uses API calls to pull in data from various online/open databases directly into your R session, without the need to separately download. When you close out your R session, these ‘temp’ files are erased, so it does not use up any of your local storage. At the end of this lesson you will learn how to save shapefiles to your computer if you do in fact want to store and use them in the future (e.g., you manipulated a data set quite a bit and don’t want to re-run the entire process every new R session). Lines tigris has many other data sets in addition to political boundaries. Today let’s work with another shapefile, importing roads for Larimer county, which returns a polyline dataset for all roads in Larimer County. roads &lt;- roads(state = &quot;CO&quot;, county = &quot;Larimer&quot;) tmap Throughout this lesson we will be using the tmap package to produce quick static or interactive maps. tmap allows for both static (“plot” mode) and interactive (“view” mode) mapping options, which you can set using the function tmap_mode() . Lets start with making quick interactive plots. Once you set the mode with tmap_mode(), every plot call to tmap after that produces a plot in that mode. Note: When you render this document to Word it will throw errors if you are trying to create interactive maps. Before rendering change “view” to “plot” in this code chunk. tmap_mode(&quot;view&quot;) Lets view our Colorado counties and Larimer County roads shapefiles. To make a “quick thematic map” in tmap you can use the qtm() function. You can also use tm_shape() plus the type of spatial layer (e.g., tm_polygons()) to add your layers to the map if you want to customize the map a little more. Notice how the two following chunks of code produce the same map, but qtm() is much more concise (but limited on customization abilities). Note that to add map elements we use +, similar to ggplot objects. #Using qtm qtm(counties)+ qtm(roads) #Using tm_shape tm_shape(counties)+ tm_polygons()+ tm_shape(roads)+ tm_lines() Rendering the map may take a little while due to relatively large size of the roads object. Mess around with this map a little bit. See that you can change the basemap, turn layers on and off, and click on features to see their attributes. Let’s inspect the spatial data sets a little more. What do you see when you run the following line of code: class(counties) sf By default, the tigris package imports spatial data in sf format, which stands for ‘simple features’. The sf package provides an easy and efficient way to work with vector data, and represents spatial features as a data.frame or tibble with a geometry column, and therefore also works well with tidyverse packages to perform manipulations like you would a data frame. For example, we are going to do an exercise for the Poudre Canyon Highway, so we want to filter out the roads data set to only those features. Using our investigative geography skills, we find the Poudre highway on the map and find out the ‘FULLNAME’ attribute is “Poudre Canyon Hwy”. We can then use that knowledge to filter() the data set to just that highway: poudre_hwy &lt;- roads %&gt;% filter(FULLNAME == &quot;Poudre Canyon Hwy&quot;) qtm(poudre_hwy) Points Most often when you are working with points, you start with an excel file or something similar that consists of the raw geographic coordinates. When you have spatial data that is not explicitly spatial yet or not in the sf format, you use the st_as_sf() function to transform it. Lets work with a couple locations along the Poudre highway, making a small data frame of their coordinates: poudre_points &lt;- data.frame(name = c(&quot;Mishawaka&quot;, &quot;Rustic&quot;, &quot;Blue Lake Trailhead&quot;), long = c(-105.35634, -105.58159, -105.85563), lat = c(40.68752, 40.69687, 40.57960)) Now convert it to an sf object, specifying the longitude and latitude column names and the CRS (Coordinate Reference System). Note that ‘x’ (longitude) always goes first followed by ‘y’ (latitude) in the coords = argument. We use the WGS84 CRS (EPSG code = 4326) here because I know the source CRS I retrieved the coordinates from, and also the GPS system often used to collect coordinates uses WGS84. poudre_points_sf &lt;- st_as_sf(poudre_points, coords = c(&quot;long&quot;, &quot;lat&quot;), crs = 4326) qtm(poudre_hwy)+ qtm(poudre_points_sf) 8.3.2 Coordinate Reference Systems Probably the most important part of working with spatial data is the coordinate reference system (CRS) that is used. In order to analyze spatial data, all objects should be in the exact same CRS. We can check a spatial object’s CRS by printing it to the console, which will return a bunch of metadata about the object. You can specifically return the CRS for sf objects with st_crs(). # see the CRS in the header metadata: counties #return just the CRS (more detailed) st_crs(counties) You can check if two sf objects have the same CRS like this: st_crs(counties) == st_crs(poudre_points_sf) Uh oh, the CRS of our points and lines doesn’t match. While tmap performs some on-the-fly transformations to map the two layers together, in order to do any analyses with these objects you’ll need to re-project one of them. You can project an sf object’s CRS to that of another with st_transform like this: poudre_points_prj &lt;- st_transform(poudre_points_sf, crs = st_crs(counties)) #Now check that they match st_crs(poudre_points_prj) == st_crs(counties) You can also project an sf object’s CRS by specifying the EPSG code. epsg.io can help you find the appropriate EPSG code for your coordinate system. For example, we know that counties is in NAD83 when we inspected the CRS above. The EPSG code for NAD83 is 4269, so we could also transform our points like this: poudre_points_prj &lt;- st_transform(poudre_points_sf, crs = 4269) #Now check that they match st_crs(poudre_points_prj) == st_crs(counties) 8.3.3 Raster Data Earlier in this lesson you downloaded a raster file for the elevation of Colorado. Make sure that file elevation.tif is in the data/ folder of your R Project, and read the raster file in the rast() from the terra package like this: elevation &lt;- rast(&quot;data/elevation.tif&quot;) Make a quick plot to see the elevation layer: qtm(elevation) By default, tmap uses a categorical symbology to color the cells by elevation. You can change that to a continuous palette with tm_raster() like this: tm_shape(elevation)+ tm_raster(style = &quot;cont&quot;, title = &quot;Elevation (m)&quot;) Let’s inspect this raster layer a little. By printing the object name to the console we see a bunch of metadata like resolution (cell size), extent, CRS, and file name. elevation We see that the CRS (coord. ref.) is in NAD83. We can also retrieve the CRS of raster objects with crs(). crs(elevation) Since this matches the CRS of our vector data we can carry on with analysis without re-projecting. However, if you did want to transform a raster object to a different CRS you would use the project() function from the terra package. terra We can use the terra package to work with raster data. For example, say we only want to see elevation along the Poudre highway. We can use crop to crop the raster to the extent of our poudre_hwy object using the ext() function to get the extent of that spatial object. Note that ‘extent’ refers to the bounding box around a spatial object. elevation_crop &lt;- crop(elevation, ext(poudre_hwy)) Lets make a final map with all the spatial data we created: qtm(elevation_crop)+ qtm(poudre_hwy)+ qtm(poudre_points_prj) 8.4 Reading and Writing Spatial Data 8.4.1 Writing spatial data All of the spatial data we’ve created are only saved as objects in our environment. To save the data to disk, the sf and terra packages have functions to do so. You are not required to save these files, but if you want to follow along with these functions save the data to the data/ folder you created at the beginning of this lesson. To save vector data with sf, use write_sf() write_sf(poudre_hwy, &quot;data/poudre_hwy.shp&quot;) write_sf(poudre_points_prj, &quot;data/poudre_points.shp&quot;) While you can give the file any name you want, note that you must put ‘.shp’ as the extension of the file. After saving the above files, check your data/ folder and notice the other auxiliary files saved with it (i.e., not just .shp). It is VERY important that whenever you share shapefiles, all the auxiliary files are saved with it, so often shapefiles are transferred via .zip folders. However, when reading shapefiles into R (see below) you only specify the file with the ‘.shp’ extension. As long as all the other auxiliary files are saved in that same folder, it will read in the shapefile correctly. To save raster data with terra use writeRaster() writeRaster(elevation_crop, &quot;data/elevation_crop.tif&quot;) Same as with the vector data, when saving raster data you must add the ‘.tif’ file extension to the name. There are various formats raster data can be stored as (e.g., ASCII, ESRI Grid) but GeoTiffs are the most common and generally easiest to deal with in R. 8.4.2 Reading Spatial Data To read in shapefiles, you use read_sf() . Note that this line of code will only run if you’ve saved your poudre_hwy object above with write_sf(). poudre_hwy &lt;- read_sf(&quot;data/poudre_hwy.shp&quot;) 8.5 Before Rendering! Before rendering this assignment to a Word document, remember Word does not work with any interactive visualizations. Therefore, go back to the beginning of your workflow where you set tmap_mode(), and instead change it to static plotting with tmap_mode(\"plot\") OR comment out tmap_mode(\"view\") as the default is “plot” mode. 8.6 Exercises Answer the following questions in your R Markdown document (also paste the questions above each answer). When your R Markdown document is complete, render it to a Word document and upload that to Canvas. 1. Filter out the counties data set to only include Larimer, Denver, and Pueblo counties. (6 pts.) 2. Lets look at the attributes for each county in our counties dataset: names(counties) We have one variable in here AWATER that is the total area of surface water each county has. Say you want to spatially visualize the variation in surface water area among counties. Looking at the arguments you can use in qtm(): ?qtm There is an argument fill =, where we can specify a variable in the dataset to color the polygons by. Use qtm() to make a map of counties that is colored by AWATER. Which county has the largest area of surface water? (7 pts.) 3. Write two lines of code to retrieve the CRS from 1) poudre_hwy and 2) elevation. (5 pts.) 4. extract() is a function in the terra package to extract raster values at specified spatial features. Run the following line of code, which will add a new column to poudre_points_prj called elevation that is the extracted elevation at each site. poudre_points_prj$elevation &lt;- extract(elevation, poudre_points_prj)[,2] Then, make a barplot that compares the elevation at each of the 3 sites. Hint : look into the use of geom_col() as opposed to geom_bar(). Which site has the highest elevation? (7 pts). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
